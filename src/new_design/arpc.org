#+title: Activity Recognition Protocol Framework

Neste documento desejo descrever e desenvolver um framework orientado a objeto em python
para experimentar com classificação de atividades humanas.

Inicialmente a ideia é criar uma classe com atributos para cada etapa do protocolo ARP,
de forma que antes de cada etapa ser executada, o valor do atributo é None, e a próxima
etapa poderá ser executada somente quando o dado em que depende estiver disponível.

# https://docs.python.org/3/tutorial/classes.html
#+begin_src python :tangle ./ARPC.py
from load_data import load_data

class Arpc:
    def __init__(self):
        self.raw_data          = None
        self.preprocessed_data = None
        self.segmented_data    = None
        self.featured_data     = None

    def load_data(self, root_dir:str, name_scheme:str):
        self.raw_data = load_data(root_dir, name_scheme)
#+end_src

#+name: arpc test
#+begin_src python
import ARPC

test = ARPC.Arpc()

# Funcionando
test.load_data('../../dataset/data',
               'Aluno<participante:\d+>/<atividade:[A-Z][a-z]*><intensidade:[A-Z][a-z]*>.txt')
test.raw_data
#+end_src

* Preprocessing data

*Falta filtrar o sensor* !

A ideia para realizar o preprocessamento dos dados irá envolver um módulo com funções
que manipulas os dados (um tipo padronizado de dados?). E na classe arpc, haverão funções para adcionarem
funções numa lista de funções que serão utilizadas para afetar os dados contidos em obj.raw_data.

# (find-fline "~/ic/src/new_design/manips.py")

# (find-fline "~/ic/src/SensorData.py")

# (find-fline "~/ic/src/new_design/arpc_utils.py")

#+transclude: [[file:./manips.py]]  :src python

#+name: teste manips
#+begin_src python
from ARPC import Arpc
import manips
from arpc_utils import aip_gen
from matplotlib import pyplot as plt

test = Arpc()
test.load_data('../../dataset/data',
               'Aluno<participante:\d+>/<atividade:[A-Z][a-z]*><intensidade:[A-Z][a-z]*>.txt')

# ! Se eu der sort no dataframe pelo tempo, não será possível identificar labels que
# ! possuem duas séries temporais embutidas, pois essa identificação é feita encontrando
# ! uma amostra onde o tempo é inferior ao tempo da amostra anterior

# (find-fline "~/ic/src/new_design/manips.py")
# (find-fline "~/ic/src/new_design/manips.py" "# DEBUGGING !")
test.raw_data
t = test.raw_data
t.loc[t['sensor'] == 'a'].drop(columns=['sensor'])
t.loc[t['sensor'] == 'a'].drop(columns=['sensor']).columns
ta = t.loc[t['sensor'] == 'a'].drop(columns=['sensor']).reset_index(drop=True)
ta
ta.sort_values(['participante', 'atividade', 'intensidade'])
tas = ta.sort_values(['participante', 'atividade', 'intensidade']).reset_index(drop=True)

# Parece estar funcionando corretamente
# Seria bom testar plotando as timeseries
manips.fix_dup(tas)
manips.fix_dup(tas, remFirst=True)

# =======================================================================

manips.fix_dup(tas)
#   File "/home/brnm/ic/src/new_design/manips.py", line 48, in fix_dup
#     df_aux['tempo'] = tempo.values
# ValueError: Length of values (602) does not match length of index (300)

manips.fix_dup(tas, remFirst=True)
#   File "/home/brnm/ic/src/new_design/manips.py", line 48, in fix_dup
#     df_aux['tempo'] = tempo.values
# ValueError: Length of values (302) does not match length of index (300)

# OFF TOPIC: Eu adoro fazer esses documentos quando
# 
#       ( fica legível e combina com como minha mente funciona, |
#         vai além de instruções, se torna plataforma para dispor o pensamento |
#         se torna uma expressão doque está passando em minha mente )
# 
#            acho muito bacana e fico grato com isso.

# Estou desfocando da tarefa de desbugar a parada
#+end_src

* Loading raw_data 

Comecei a me confundir muito com como eu vou tanglar isso aqui.

# (find-fline "~/ic/src/new_design/load_data.py")

#+transclude: [[file:./load_data.py]]  :src python

#+name: test load_data
#+begin_src python
import load_data
from pprint import pprint

# Funcionando como esperado
# (find-fline "~/ic/src/new_design/load_data.py" "def process_name_scheme")
name_scheme = "Aluno<participante:\d+>/<atividade:[A-Z][a-z]*><intensidade:[A-Z][a-z]*>.txt"
pprint(load_data.process_name_scheme(name_scheme))
r = load_data.process_name_scheme(name_scheme)


# Funcionando como esperado
# (find-fline "~/ic/src/new_design/load_data.py" "def list_files")
load_data.list_files('../../dataset/data/', r[0])

# Funcionando
# (find-fline "~/ic/src/new_design/load_data.py" "load_data")
load_data.load_data('../../dataset/data/', "Aluno<participante:\d+>/<atividade:[A-Z][a-z]*><intensidade:[A-Z][a-z]*>.txt")
#+end_src

** Como cheguei nesta solução

Eu estava utilizando um esquema com list comprehensions para especificar o nome dos arquivos
a serem carregados na memória pelo pandas.
No momento em que os dados eram carregados eu adcionava valores para novas colunas que
indicavam qual era o participante, qual a atividade e qual a intensidade.
Esses campos eram futuramente utilizados para selecionar quais dados seriam utilizados nas
operações.

#+name: Código antigo responsável por carregar dados na memória
#+begin_src python

# for loading data
atividades   = ['Andando', 'Sentado', 'Deitado']
intensidades = ['Leve', 'Moderado', 'Vigoroso']

p_dir        = ['Aluno'+str(i+1) for i in range(11)]

    def __init__(self, dataset_dir  = '~/ic/dataset/data/', extension='.txt'):
        df = pd.DataFrame(columns=['x', 'y', 'z', 'tempo', 'sensor'])

        full_paths = {}
        for p in p_dir:
            full_paths[p] = {}
            for a in atividades:
                full_paths[p][a] = {}
                for i in intensidades:
                    full_paths[p][a][i] = dataset_dir + p + '/' + a + i + extension

        participantes = list(range(len(p_dir)))

        # Loading data
        for p, pn in zip(p_dir, participantes):
            for a in atividades:
                for i in intensidades:
                    df_r = pd.read_csv(full_paths[p][a][i], delim_whitespace=True,
                                    names=['x', 'y', 'z', 'tempo', 'sensor'])\
                            .assign(Atividade = a,
                                    Intensidade = i,
                                    Participante = pn)

                    df_r = df_r.loc[df_r['sensor'] == 'a']

                    df = pd.concat([df, df_r], ignore_index=True)

        self.data = df
        self.participantes = participantes
#+end_src

As informações necessárias para realizar o carregamento dos dados são:
1. O diretório root onde os arquivos se encontram
2. O esquema de nomes dos arquivos

O esquema de nomes dos arquivos informa metadados sobre os dados contidos no arquivo.
Como este esquema poderia ser informado de forma que automatize a inserção dos metadados no
DataFrame do pandas?

#+begin_src python :session name_scheme
root_dir = "~/ic/dataset/"
scheme   = "Aluno<participante:\d+>/<atividade:[A-Z]\w*><intensidade:[A-Z]\w*>.csv"
#+end_src

Deste esquema deveria ser possível inferir que ao carregar um arquivo na memória,
determinados campos que fazem parte do nome do arquivo servirão para prenhcer novas
colunas que serão criadas.

Então da string 'scheme' deve ser possível extrair:
#+begin_src python
[('participante', r'\d+'     ),   # Nome de cada campo que será adcionado nos dados
 ('atividade',    r'[A-Z]\w*'),   # junto com a regexp que irá buscar o valor no nome
 ('intensidade',  r'[A-Z]\w*')]

r'Aluno\d+/[A-Z]\w*[A-Z]\w*.csv' # Para selecionar todos os arquivos que serão carregados
                                 # na memória
#+end_src

A segunda regexp deve ser usada para adquirir a lista de todos os arquivos que serão
carregados na memória a partir do 'root_dir'.
# https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory
# https://docs.python.org/2/library/os.html#os.listdir
# https://stackoverflow.com/questions/2212643/python-recursive-folder-read
# https://docs.python.org/3/library/os.html#os.walk
