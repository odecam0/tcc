#+title: Activity Recognition Protocol Framework


! Preciso pensar em uma maneira de deixar esta ferramenta independente dos nomes que
! forem utilizados para cada coluna.


Neste documento desejo descrever e desenvolver um framework orientado a objeto em python
para experimentar com classificação de atividades humanas.

Inicialmente a ideia é criar uma classe com atributos para cada etapa do protocolo ARP,
de forma que antes de cada etapa ser executada, o valor do atributo é None, e a próxima
etapa poderá ser executada somente quando o dado em que depende estiver disponível.

#+transclude: [[file:ARPC.py]]  :src python

# (find-icfile "src/new_design/")
# (find-icfile "src/new_design/ARPC.py")
# (find-icfile "src/new_design/arpc_utils.py")

#+name: arpc test
#+begin_src python
from test import test
from arpc_plot import plot_all
from arpc_utils import get_acc_data, get_gyr_data
test.raw_data
test.preprocessed_data

plot_all(test.raw_data, participantes=['1'])
plot_all(get_acc_data(test.raw_data), participantes=['1'])
plot_all(get_gyr_data(test.raw_data), participantes=['1'])

plot_all(test.preprocessed_data, participantes=['1'])
plot_all(get_acc_data(test.preprocessed_data), participantes=['1'])
plot_all(get_gyr_data(test.preprocessed_data), participantes=['1'])
#+end_src

* Classification

A classificação normalmente é feita em algumas etapas

* Features

# (find-icfile "src/new_design/")
# (find-icfile "src/new_design/arpc_features.py")

Tenho janelas com N colunas, um subconjunto dessas colunas será utilizado para gerar as features.

# Esse demorou pra sair..

#+begin_src python
# Até este ponto, tenho diversas listas com janelas, separadas por label em um dicionário.
# Aqui, tento transformar estes dados em 1 DataFrame só, com features, calculadas em cada uma
# das janelas..

# (find-icfile "src/new_design/ARPC.py" "def set_features")
# (find-icfile "src/new_design/arpc_features.py" "def calc_feature")
# (find-icfile "src/new_design/arpc_features.py" "def merge_features")

# Funcionou :)
from test import test
import numpy as np
test.set_features([np.mean])
test.set_features([np.mean], columns=['x', 'y', 'z', 'module'])
test.set_features([np.mean, np.std], columns=['x', 'y', 'z', 'module'])
test.featured_data

test.raw_data.x
np.std(test.raw_data.x)
#+end_src

https://stackoverflow.com/questions/684171/how-to-re-import-an-updated-package-while-in-python-interpreter
from importlib import reload
reload(<module>)

* Windowing

A ideia aqui é permitir realizar operações nos dados de cada janela.
O janelamento em pandas é feito utilizando uma classe que é retornada por uma função..
Mas eu tive dificuldade de acessar e modificar os dados. Acredito que isso ocorre pela forma que o janelamento
é implementado, utilizando uma classe indexer.. Não são criados novos objetos para cada janela, e sim uma sequencia de
indices do dataframe para cada janela. Isso é bem inteligente, mas vou ter que driblar isso para conseguir fazer a
manipulação de dados que gostaria.

Futuramente na etapa de extração das features, algo ocorre. 
Oque ocorre? Eu utilizo os métodos do Window para adquirir essas features.
Que que tem? Para modificar os dados das janelas perco acesso ao objeto Window.
Vai ter que começar a extrair as features na mão?
Isso, ou dar um jeito de recriar o Window com os dados modificados.

De qualquer forma, a forma que eu estava realizando a extração de características era bem ruim.
Então reescreve-la é o correto.

# (find-icfile "src/new_design/")
# (find-icfile "src/new_design/arpc_window.py")
# (find-icfile "src/new_design/ARPC.py")

#+name: testing window
#+begin_src python
import ARPC
import manips
from arpc_plot import plot_all
from arpc_utils import get_acc_data, get_gyr_data

test = ARPC.Arpc()

# Funcionando
test.load_data('../../dataset/data',
               'Aluno<participante:\d+>/<atividade:[A-Z][a-z]*><intensidade:[A-Z][a-z]*>.txt')
test.raw_data

test.add_manip(get_acc_data)
test.add_manip(lambda x: manips.fix_dup(x, remFirst=True))

classes = [(1., 'Deitado', 'Moderado')]
classes += [(4., 'Deitado', i) for i in ['Leve', 'Moderado', 'Vigoroso']]
classes += [(7., 'Deitado', i) for i in ['Leve', 'Moderado', 'Vigoroso']]
test.add_manip(lambda x: manips.rotate_class(x, classes, [0, 0, 1]))

test.add_manip(manips.remove_outliers)
test.add_manip(manips.remove_beginning)
test.add_manip(manips.scale_data)

test.do_manip()

test.preprocessed_data

test.set_windows()
# test.segmented_data # demora mto pra printar
test.segmented_data.keys()
test.segmented_data['1AndandoLeve']
len(test.segmented_data['1AndandoLeve'])
test.segmented_data['1AndandoLeve'][0]  

from TimeWarpWindow import warp_window

# (find-icfile "src/new_design/TimeWarpWindow.py")
# Não usei lambda porque utilizo o nome da função no src
def timewarped(df):
    result, _ = warp_window(df, 3)
    return result

# Funcionando
test.apply_each_window(funcs=[timewarped])
test.segmented_data.keys()
test.segmented_data['1AndandoLeve'][0]
test.segmented_data['timewarped_1AndandoLeve'][0]
#+end_src

# Exemplo de uma janela:
#           x         y         z  tempo sensor atividade intensidade participante
# 0  1.660363 -0.374177 -1.283886  10093      a   Andando        Leve            1
# 1  1.646849 -0.378761 -1.285337  10289      a   Andando        Leve            1
# 2  1.655668 -0.377730 -1.281915  10489      a   Andando        Leve            1
# 3  1.647765 -0.376928 -1.287204  10691      a   Andando        Leve            1
# 4  1.635969 -0.366383 -1.289797  10890      a   Andando        Leve            1
# 5  1.652232 -0.378991 -1.289797  11090      a   Andando        Leve            1
# 6  1.645475 -0.376469 -1.289693  11291      a   Andando        Leve            1
# 7  1.666433 -0.387243 -1.284923  11488      a   Andando        Leve            1
# 8  1.642611 -0.377959 -1.282330  11687      a   Andando        Leve            1
# 9  1.651888 -0.379105 -1.285649  11890      a   Andando        Leve            1

Ainda estou confuso sobre se ARPC.segmented_data deve possuir dataframes ou matrizes numpy...
Algo me diz que deve ser matrizes numpy...

* Plotting

# (find-icfile "src/new_design/arpc_plot.py" "def plot_all")
# (defun p () (interactive) (find-icfile "src/new_design/arpc_plot.py" "def plot_all"))

* Preprocessing data

A ideia para realizar o preprocessamento dos dados irá envolver um módulo com funções
que manipulas os dados (um tipo padronizado de dados?). E na classe arpc, haverão funções para adcionarem
funções numa lista de funções que serão utilizadas para afetar os dados contidos em obj.raw_data.

# (find-fline "~/ic/src/new_design/manips.py")

# (find-fline "~/ic/src/SensorData.py")
# (find-fline "~/ic/src/SensorData.el")

# (find-fline "~/ic/src/new_design/arpc_utils.py")

#+transclude: [[file:./manips.py]]  :src python

** scale_data

# (defun m () (interactive) (find-icfile "src/new_design/manips.py" "def scale_data"))

#+begin_src python
from ARPC import Arpc
import manips
from arpc_utils import aip_gen
from matplotlib import pyplot as plt
from arpc_plot import plot_all

test = Arpc()
test.load_data('../../dataset/data',
               'Aluno<participante:\d+>/<atividade:[A-Z][a-z]*><intensidade:[A-Z][a-z]*>.txt')

def sorted_sensora_df(df):
    return df.loc[df['sensor'] == 'a']\
             .drop(columns=['sensor'])\
             .sort_values(['participante', 'atividade', 'intensidade'])\
             .reset_index(drop=True)

manips.scale_data(test.raw_data)
test.raw_data
#+end_src

** remove_beginning

# (defun m () (interactive) (find-icfile "src/new_design/manips.py" "def remove_beginning"))

#+begin_src python
from ARPC import Arpc
import manips
from arpc_utils import aip_gen
from matplotlib import pyplot as plt
from arpc_plot import plot_all

test = Arpc()
test.load_data('../../dataset/data',
               'Aluno<participante:\d+>/<atividade:[A-Z][a-z]*><intensidade:[A-Z][a-z]*>.txt')

def sorted_sensora_df(df):
    return df.loc[df['sensor'] == 'a']\
             .drop(columns=['sensor'])\
             .sort_values(['participante', 'atividade', 'intensidade'])\
             .reset_index(drop=True)

manips.remove_beginning(test.raw_data)
#+end_src

** remove_outliers

# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html
# fixing remove_outliers

# (defun m () (interactive) (find-icfile "src/new_design/manips.py" "def remove_outliers"))

#+begin_src python
from ARPC import Arpc
import manips
from arpc_utils import aip_gen
from matplotlib import pyplot as plt
from arpc_plot import plot_all

test = Arpc()
test.load_data('../../dataset/data',
               'Aluno<participante:\d+>/<atividade:[A-Z][a-z]*><intensidade:[A-Z][a-z]*>.txt')

def sorted_sensora_df(df):
    return df.loc[df['sensor'] == 'a']\
             .drop(columns=['sensor'])\
             .sort_values(['participante', 'atividade', 'intensidade'])\
             .reset_index(drop=True)

for i in aip_gen(test.raw_data.sort_values(['participante', 'atividade', 'intensidade'])):
    atividade = i.atividade.iloc[0]
    intensidade = i.intensidade.iloc[0]
    participante = i.participante.iloc[0]
    print(participante, atividade, intensidade)
    i.loc[:, ['x', 'y', 'z']].boxplot()
    break

plt.show()

df = manips.remove_outliers(sorted_sensora_df(test.raw_data))

for i in aip_gen(df):
    atividade = i.atividade.iloc[0]
    intensidade = i.intensidade.iloc[0]
    participante = i.participante.iloc[0]
    print(participante, atividade, intensidade)
    i.loc[:, ['x', 'y', 'z']].boxplot()
    break

plt.show() # Penso que eu deveria entender matemáticamente este método de remoção de outliers

# Ta funcionando
#+end_src

** Adapting rotate_class

#+begin_src python
from ARPC import Arpc
import manips
from arpc_utils import aip_gen
from matplotlib import pyplot as plt
from arpc_plot import plot_all

test = Arpc()
test.load_data('../../dataset/data',
               'Aluno<participante:\d+>/<atividade:[A-Z][a-z]*><intensidade:[A-Z][a-z]*>.txt')

def sorted_sensora_df(df):
    return df.loc[df['sensor'] == 'a']\
             .drop(columns=['sensor'])\
             .sort_values(['participante', 'atividade', 'intensidade'])\
             .reset_index(drop=True)

df = sorted_sensora_df(test.raw_data)
plot_all(df, participantes=['1'])
             # Mudar para passar int

# (find-fline "~/ic/src/new_design/manips.py")
# (find-fline "~/ic/src/new_design/manips.py" "def rotate_class")
# (defun m () (interactive) (find-fline "~/ic/src/new_design/manips.py" "def rotate_class"))
# (find-icfile "src/SensorData.el" ";; Rotacionando os dados")

manips.rotate_class(test.raw_data, [('1', 'Deitado', 'Moderado')], [0,0,1])
df = manips.rotate_class(sorted_sensora_df(test.raw_data),   
                    [('2', 'Deitado', 'Moderado')], [0,0,1])

plot_all(df, participantes=['2'])
#+end_src

** Fixing fix_dups
#+name: teste manips.fix_dups
#+begin_src python
from ARPC import Arpc
import manips
from arpc_utils import aip_gen
from matplotlib import pyplot as plt
from arpc_plot import plot_all

test = Arpc()
test.load_data('../../dataset/data',
               'Aluno<participante:\d+>/<atividade:[A-Z][a-z]*><intensidade:[A-Z][a-z]*>.txt')

# ! Se eu der sort no dataframe pelo tempo, não será possível identificar labels que
# ! possuem duas séries temporais embutidas, pois essa identificação é feita encontrando
# ! uma amostra onde o tempo é inferior ao tempo da amostra anterior

# (find-fline "~/ic/src/new_design/manips.py")
# (find-fline "~/ic/src/new_design/manips.py" "# DEBUGGING !")
test.raw_data
t = test.raw_data
t.loc[t['sensor'] == 'a'].drop(columns=['sensor'])
t.loc[t['sensor'] == 'a'].drop(columns=['sensor']).columns
ta = t.loc[t['sensor'] == 'a'].drop(columns=['sensor']).reset_index(drop=True)
ta
ta.sort_values(['participante', 'atividade', 'intensidade'])
tas = ta.sort_values(['participante', 'atividade', 'intensidade']).reset_index(drop=True)

manips.fix_dup(tas)
manips.fix_dup(tas, remFirst=True)

plot_all(manips.fix_dup(tas), participantes=['1'])                # Deu errado
plot_all(manips.fix_dup(tas, remFirst=True), participantes=['1']) # Deu certo

# =======================================================================

manips.fix_dup(tas)
#   File "/home/brnm/ic/src/new_design/manips.py", line 48, in fix_dup
#     df_aux['tempo'] = tempo.values
# ValueError: Length of values (602) does not match length of index (300)

manips.fix_dup(tas, remFirst=True)
#   File "/home/brnm/ic/src/new_design/manips.py", line 48, in fix_dup
#     df_aux['tempo'] = tempo.values
# ValueError: Length of values (302) does not match length of index (300)

# OFF TOPIC: Eu adoro fazer esses documentos quando
# 
#       ( fica legível e combina com como minha mente funciona, |
#         vai além de instruções, se torna plataforma para dispor o pensamento |
#         se torna uma expressão doque está passando em minha mente )
# 
#            acho muito bacana e fico grato com isso.

# Estou desfocando da tarefa de desbugar a parada
#+end_src

* Loading raw_data 

Comecei a me confundir muito com como eu vou tanglar isso aqui.

# (find-fline "~/ic/src/new_design/load_data.py")

#+transclude: [[file:./load_data.py]]  :src python

#+name: test load_data
#+begin_src python
import load_data
from pprint import pprint

# Funcionando como esperado
# (find-fline "~/ic/src/new_design/load_data.py" "def process_name_scheme")
name_scheme = "Aluno<participante:\d+>/<atividade:[A-Z][a-z]*><intensidade:[A-Z][a-z]*>.txt"
pprint(load_data.process_name_scheme(name_scheme))
r = load_data.process_name_scheme(name_scheme)


# Funcionando como esperado
# (find-fline "~/ic/src/new_design/load_data.py" "def list_files")
load_data.list_files('../../dataset/data/', r[0])

# Funcionando
# (find-fline "~/ic/src/new_design/load_data.py" "load_data")
load_data.load_data('../../dataset/data/', "Aluno<participante:\d+>/<atividade:[A-Z][a-z]*><intensidade:[A-Z][a-z]*>.txt")
#+end_src

** Como cheguei nesta solução

Eu estava utilizando um esquema com list comprehensions para especificar o nome dos arquivos
a serem carregados na memória pelo pandas.
No momento em que os dados eram carregados eu adcionava valores para novas colunas que
indicavam qual era o participante, qual a atividade e qual a intensidade.
Esses campos eram futuramente utilizados para selecionar quais dados seriam utilizados nas
operações.

#+name: Código antigo responsável por carregar dados na memória
#+begin_src python

# for loading data
atividades   = ['Andando', 'Sentado', 'Deitado']
intensidades = ['Leve', 'Moderado', 'Vigoroso']

p_dir        = ['Aluno'+str(i+1) for i in range(11)]

    def __init__(self, dataset_dir  = '~/ic/dataset/data/', extension='.txt'):
        df = pd.DataFrame(columns=['x', 'y', 'z', 'tempo', 'sensor'])

        full_paths = {}
        for p in p_dir:
            full_paths[p] = {}
            for a in atividades:
                full_paths[p][a] = {}
                for i in intensidades:
                    full_paths[p][a][i] = dataset_dir + p + '/' + a + i + extension

        participantes = list(range(len(p_dir)))

        # Loading data
        for p, pn in zip(p_dir, participantes):
            for a in atividades:
                for i in intensidades:
                    df_r = pd.read_csv(full_paths[p][a][i], delim_whitespace=True,
                                    names=['x', 'y', 'z', 'tempo', 'sensor'])\
                            .assign(Atividade = a,
                                    Intensidade = i,
                                    Participante = pn)

                    df_r = df_r.loc[df_r['sensor'] == 'a']

                    df = pd.concat([df, df_r], ignore_index=True)

        self.data = df
        self.participantes = participantes
#+end_src

As informações necessárias para realizar o carregamento dos dados são:
1. O diretório root onde os arquivos se encontram
2. O esquema de nomes dos arquivos

O esquema de nomes dos arquivos informa metadados sobre os dados contidos no arquivo.
Como este esquema poderia ser informado de forma que automatize a inserção dos metadados no
DataFrame do pandas?

#+begin_src python :session name_scheme
root_dir = "~/ic/dataset/"
scheme   = "Aluno<participante:\d+>/<atividade:[A-Z]\w*><intensidade:[A-Z]\w*>.csv"
#+end_src

Deste esquema deveria ser possível inferir que ao carregar um arquivo na memória,
determinados campos que fazem parte do nome do arquivo servirão para prenhcer novas
colunas que serão criadas.

Então da string 'scheme' deve ser possível extrair:
#+begin_src python
[('participante', r'\d+'     ),   # Nome de cada campo que será adcionado nos dados
 ('atividade',    r'[A-Z]\w*'),   # junto com a regexp que irá buscar o valor no nome
 ('intensidade',  r'[A-Z]\w*')]

r'Aluno\d+/[A-Z]\w*[A-Z]\w*.csv' # Para selecionar todos os arquivos que serão carregados
                                 # na memória
#+end_src

A segunda regexp deve ser usada para adquirir a lista de todos os arquivos que serão
carregados na memória a partir do 'root_dir'.
# https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory
# https://docs.python.org/2/library/os.html#os.listdir
# https://stackoverflow.com/questions/2212643/python-recursive-folder-read
# https://docs.python.org/3/library/os.html#os.walk
